{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB-r2iv9sPpr",
        "outputId": "3fe618f4-1543-41fb-fb72-74194ee2bf2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.123.10)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.40.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Collecting groq\n",
            "  Downloading groq-1.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.50.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Downloading groq-1.0.0-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn pydantic python-dotenv groq\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install groq python-dotenv\n",
        "\n"
      ],
      "metadata": {
        "id": "Sc9TR8r1tAcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"enter api box\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpderl-XtjGn",
        "outputId": "e73ede34-5cc1-4470-a093-b020c1bfb766"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter api box··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "\n",
        "# List all currently available models\n",
        "models = client.models.list().data\n",
        "model_ids = [m.id for m in models]\n",
        "\n",
        "print(\"✅ Available Groq Models:\")\n",
        "for mid in model_ids:\n",
        "    print(\"-\", mid)\n",
        "  # you can also use: llama3-8b-8192, mixtral-8x7b-32768\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UyK9eJouxHA",
        "outputId": "b1b4b26e-3fa1-4e86-cef6-f9d669a545fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Available Groq Models:\n",
            "- llama-3.1-8b-instant\n",
            "- meta-llama/llama-prompt-guard-2-86m\n",
            "- llama-3.3-70b-versatile\n",
            "- qwen/qwen3-32b\n",
            "- meta-llama/llama-prompt-guard-2-22m\n",
            "- canopylabs/orpheus-arabic-saudi\n",
            "- whisper-large-v3\n",
            "- groq/compound-mini\n",
            "- moonshotai/kimi-k2-instruct\n",
            "- openai/gpt-oss-120b\n",
            "- meta-llama/llama-4-scout-17b-16e-instruct\n",
            "- openai/gpt-oss-20b\n",
            "- meta-llama/llama-4-maverick-17b-128e-instruct\n",
            "- whisper-large-v3-turbo\n",
            "- canopylabs/orpheus-v1-english\n",
            "- openai/gpt-oss-safeguard-20b\n",
            "- allam-2-7b\n",
            "- meta-llama/llama-guard-4-12b\n",
            "- moonshotai/kimi-k2-instruct-0905\n",
            "- groq/compound\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PREFERRED_MODELS = [\n",
        "    # best quality first\n",
        "    \"mixtral-8x7b-32768\",\n",
        "    \"llama3-70b-8192\",  # old (may not exist)\n",
        "    \"llama-3.1-70b-versatile\",  # old (may not exist)\n",
        "\n",
        "    # fallback fast\n",
        "    \"llama3-8b-8192\",\n",
        "    \"gemma2-9b-it\"\n",
        "]\n",
        "\n",
        "def choose_model():\n",
        "    available = {m.id for m in client.models.list().data}\n",
        "\n",
        "    for m in PREFERRED_MODELS:\n",
        "        if m in available:\n",
        "            return m\n",
        "\n",
        "    # if none matched, choose the first available model\n",
        "    return list(available)[0]\n",
        "\n",
        "DEFAULT_MODEL = choose_model()\n",
        "print(\"✅ Selected model:\", DEFAULT_MODEL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9P2gI0iwo9f",
        "outputId": "c52f7f49-5adc-4e15-8530-442d85fa4af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Selected model: moonshotai/kimi-k2-instruct-0905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def call_groq(prompt: str, model: str = None, temperature: float = 0.3, max_tokens: int = 1024):\n",
        "    if model is None:\n",
        "        model = DEFAULT_MODEL\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "    return resp.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "xt9QccNWws65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "\n",
        "# -------- ROOT PROMPT --------\n",
        "ROOT_PROMPT = \"\"\"\n",
        "SYSTEM (Root Prompt):\n",
        "You are an expert AI assistant.\n",
        "\n",
        "Rules:\n",
        "- Be accurate, structured, and helpful.\n",
        "- Do not hallucinate. If unsure, say so.\n",
        "- Use RGC framework (Role, Goal, Context).\n",
        "- If unclear: use Question Refinement Pattern.\n",
        "- If important: apply Cognitive Verifier Pattern.\n",
        "- Think step-by-step internally (do not show hidden chain-of-thought).\n",
        "- Provide final reasoning as short clear steps.\n",
        "\n",
        "Output format:\n",
        "Summary:\n",
        "Steps:\n",
        "Final Answer:\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# -------- PERSONA --------\n",
        "@dataclass\n",
        "class Persona:\n",
        "    name: str\n",
        "    expertise: str\n",
        "    tone: str\n",
        "    style: str\n",
        "\n",
        "    def render(self) -> str:\n",
        "        return f\"\"\"\n",
        "PERSONA:\n",
        "You are {self.name}.\n",
        "Expertise: {self.expertise}\n",
        "Tone: {self.tone}\n",
        "Style: {self.style}\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# -------- RGC --------\n",
        "def build_rgc(role: str, goal: str, context: str) -> str:\n",
        "    return f\"\"\"\n",
        "[RGC]\n",
        "ROLE: {role}\n",
        "GOAL: {goal}\n",
        "CONTEXT: {context}\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# -------- Prompting Styles --------\n",
        "def zero_shot(task: str) -> str:\n",
        "    return f\"ZERO-SHOT TASK:\\n{task}\".strip()\n",
        "\n",
        "\n",
        "def one_shot(task: str, example_input: str, example_output: str) -> str:\n",
        "    return f\"\"\"\n",
        "ONE-SHOT TASK:\n",
        "Example:\n",
        "Input: {example_input}\n",
        "Output: {example_output}\n",
        "\n",
        "Now solve:\n",
        "{task}\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def few_shot(task: str, examples: List[Dict[str, str]]) -> str:\n",
        "    examples_text = \"\\n\\n\".join(\n",
        "        [f\"Example {i+1}:\\nInput: {ex['input']}\\nOutput: {ex['output']}\"\n",
        "         for i, ex in enumerate(examples)]\n",
        "    )\n",
        "    return f\"\"\"\n",
        "FEW-SHOT TASK:\n",
        "{examples_text}\n",
        "\n",
        "Now solve:\n",
        "{task}\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# -------- Question Refinement Pattern --------\n",
        "def question_refinement(user_task: str) -> str:\n",
        "    return f\"\"\"\n",
        "[QUESTION REFINEMENT PATTERN]\n",
        "Original request: {user_task}\n",
        "\n",
        "Do:\n",
        "1) Rewrite it into a clear single sentence.\n",
        "2) List missing details.\n",
        "3) Ask max 2 clarifying questions.\n",
        "4) Provide a draft response with assumptions.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# -------- Cognitive Verifier Pattern --------\n",
        "def cognitive_verifier(draft_answer: str) -> str:\n",
        "    return f\"\"\"\n",
        "[COGNITIVE VERIFIER PATTERN]\n",
        "Draft answer:\n",
        "{draft_answer}\n",
        "\n",
        "Verify:\n",
        "1) Identify assumptions\n",
        "2) Check correctness\n",
        "3) Handle edge cases\n",
        "4) Improve quality and clarity\n",
        "\n",
        "Return strictly:\n",
        "Summary:\n",
        "Steps:\n",
        "Final Answer:\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# -------- Prompt Engine --------\n",
        "class PromptEngine:\n",
        "    def __init__(self, persona: Persona):\n",
        "        self.persona = persona\n",
        "\n",
        "    def build_prompt(\n",
        "        self,\n",
        "        user_task: str,\n",
        "        role: str,\n",
        "        goal: str,\n",
        "        context: str,\n",
        "        mode: str = \"zero\",\n",
        "        examples: Optional[List[Dict[str, str]]] = None,\n",
        "        refinement: bool = True\n",
        "    ) -> str:\n",
        "\n",
        "        persona_block = self.persona.render()\n",
        "        rgc_block = build_rgc(role, goal, context)\n",
        "\n",
        "        if mode == \"zero\":\n",
        "            task_block = zero_shot(user_task)\n",
        "        elif mode == \"one\":\n",
        "            if not examples or len(examples) < 1:\n",
        "                raise ValueError(\"One-shot requires 1 example\")\n",
        "            ex = examples[0]\n",
        "            task_block = one_shot(user_task, ex[\"input\"], ex[\"output\"])\n",
        "        elif mode == \"few\":\n",
        "            if not examples or len(examples) < 2:\n",
        "                raise ValueError(\"Few-shot requires >=2 examples\")\n",
        "            task_block = few_shot(user_task, examples)\n",
        "        else:\n",
        "            raise ValueError(\"mode must be: zero / one / few\")\n",
        "\n",
        "        refine_block = question_refinement(user_task) if refinement else \"\"\n",
        "\n",
        "        full_prompt = f\"\"\"\n",
        "{ROOT_PROMPT}\n",
        "\n",
        "{persona_block}\n",
        "\n",
        "{rgc_block}\n",
        "\n",
        "{task_block}\n",
        "\n",
        "{refine_block}\n",
        "\n",
        "Return final output exactly in:\n",
        "Summary:\n",
        "Steps:\n",
        "Final Answer:\n",
        "\"\"\".strip()\n",
        "\n",
        "        return full_prompt\n"
      ],
      "metadata": {
        "id": "J9VPxdOHu34D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_groq(prompt: str, model: str = DEFAULT_MODEL, temperature: float = 0.3, max_tokens: int = 1024):\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "    return resp.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "kA09RMUou8tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persona = Persona(\n",
        "    name=\"Astra\",\n",
        "    expertise=\"AI/ML + software engineering + prompt engineering\",\n",
        "    tone=\"friendly, professional, structured\",\n",
        "    style=\"mentor-like: explain simple first, then deep with examples\"\n",
        ")\n",
        "\n",
        "engine = PromptEngine(persona)\n",
        "\n",
        "def generate_answer(\n",
        "    user_task: str,\n",
        "    role: str,\n",
        "    goal: str,\n",
        "    context: str,\n",
        "    mode: str = \"zero\",\n",
        "    examples: Optional[List[Dict[str, str]]] = None,\n",
        "    refinement: bool = True,\n",
        "    verifier: bool = True,\n",
        "    model: str = DEFAULT_MODEL\n",
        "):\n",
        "    # build prompt\n",
        "    prompt = engine.build_prompt(\n",
        "        user_task=user_task,\n",
        "        role=role,\n",
        "        goal=goal,\n",
        "        context=context,\n",
        "        mode=mode,\n",
        "        examples=examples,\n",
        "        refinement=refinement\n",
        "    )\n",
        "\n",
        "    # first pass (draft)\n",
        "    draft = call_groq(prompt, model=model)\n",
        "\n",
        "    # second pass (verification)\n",
        "    if verifier:\n",
        "        verify_prompt = cognitive_verifier(draft)\n",
        "        final_answer = call_groq(verify_prompt, model=model)\n",
        "    else:\n",
        "        final_answer = draft\n",
        "\n",
        "    return {\n",
        "        \"prompt_used\": prompt,\n",
        "        \"draft_answer\": draft,\n",
        "        \"final_answer\": final_answer\n",
        "    }\n"
      ],
      "metadata": {
        "id": "NdE7up1uu-__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = generate_answer(\n",
        "    user_task=\"Explain linear search in easy way and provide Java code.\",\n",
        "    role=\"You are a DSA mentor\",\n",
        "    goal=\"Teach a beginner clearly with steps and example\",\n",
        "    context=\"Student knows loops, arrays, and if-else\",\n",
        "    mode=\"zero\",\n",
        "    refinement=True,\n",
        "    verifier=True,\n",
        "    model=DEFAULT_MODEL\n",
        ")\n",
        "\n",
        "print(\"✅ FINAL ANSWER:\\n\")\n",
        "print(result[\"final_answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emrFjAIFvBdt",
        "outputId": "2e89bac3-8c8b-4620-b74c-b95d1597f540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FINAL ANSWER:\n",
            "\n",
            "Summary:\n",
            "Explain how linear search works in plain language and give a ready-to-run Java example.\n",
            "\n",
            "Steps:\n",
            "1. Clarify & simplify the request.  \n",
            "2. List what we still don’t know (none critical for a basic demo).  \n",
            "3. Supply a concise explanation + code under common assumptions.\n",
            "\n",
            "Final Answer:\n",
            "Rewritten request: “Teach a beginner what linear search is and show a simple Java program that looks for a number in an array.”\n",
            "\n",
            "Missing details (non-blocking):  \n",
            "- Should the program handle input from the keyboard?  \n",
            "- Do we need to search other data types (strings, objects)?\n",
            "\n",
            "Assumptions made:  \n",
            "- Fixed array of ints, hard-coded for clarity.  \n",
            "- Search key also hard-coded.  \n",
            "- Print index if found, “not found” otherwise.\n",
            "\n",
            "Plain-English explanation:  \n",
            "Imagine you have a row of numbered boxes. Start at the left, open each box one by one, and stop when you find the number you want. That’s linear search; at worst you check every box.\n",
            "\n",
            "Java code:\n",
            "```java\n",
            "public class LinearSearchDemo {\n",
            "    public static int linearSearch(int[] arr, int target) {\n",
            "        for (int i = 0; i < arr.length; i++) {\n",
            "            if (arr[i] == target) {\n",
            "                return i;          // found: return index\n",
            "            }\n",
            "        }\n",
            "        return -1;                   // not found\n",
            "    }\n",
            "\n",
            "    public static void main(String[] args) {\n",
            "        int[] numbers = {4, 7, 9, 3, 8};\n",
            "        int key = 9;\n",
            "\n",
            "        int index = linearSearch(numbers, key);\n",
            "        if (index != -1) {\n",
            "            System.out.println(\"Found \" + key + \" at index \" + index);\n",
            "        } else {\n",
            "            System.out.println(key + \" not found.\");\n",
            "        }\n",
            "    }\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NchAWrM2vDwM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}